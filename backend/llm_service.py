import requests
import json
from backend.config import OLLAMA_API_URL, MODEL_NAME, SYSTEM_PROMPT, SUMMARY_INTERVAL

# === å…¨å±€å­˜å‚¨ - å‚ä¸è€…ä¼šè¯æ•°æ®éš”ç¦» ===
# Key: participant_id
# Value: {'history': [...], 'summary': '...', 'turn_count': 0, 'sentiment_scores': []}
session_data = {}


def get_session(participant_id: str) -> dict:
    """è·å–æˆ–åˆå§‹åŒ–å‚ä¸è€…çš„ä¼šè¯æ•°æ®"""
    if participant_id not in session_data:
        session_data[participant_id] = {
            'history': [],
            'summary': "",
            'full_prompt': "",
            'turn_count': 0,  # <--- å›åˆè®¡æ•°å™¨
            'sentiment_scores': []  # <--- æƒ…ç»ªå¾—åˆ†å ä½ç¬¦åˆ—è¡¨
        }
    return session_data[participant_id]


def clear_session(participant_id: str) -> bool:
    """æ¸…é™¤ç‰¹å®šå‚ä¸è€…çš„ä¼šè¯å†å²å’Œæ‘˜è¦ (ç”¨äºæ–°å®éªŒå¼€å§‹æ—¶)"""
    if participant_id in session_data:
        del session_data[participant_id]
        print(f"ğŸ§¹ Session cleared for PID {participant_id}")
        return True
    return False


def generate_summary(session: dict):
    """ç”Ÿæˆè¿‘æœŸå¯¹è¯çš„ç®€çŸ­æ‘˜è¦ (ç”¨äºä¸Šä¸‹æ–‡è®°å¿†)"""

    conversation_history = session['history']
    summary_memory = session['summary']

    recent_dialogue = "\n".join(
        [f"{m['role'].capitalize()}: {m['content']}" for m in conversation_history[-10:]]
    )

    summary_prompt = f"""
Please summarize the following conversation into a concise summary of no more than 150 words. 
Focus on the user's main emotions, topics, and intents. Keep the summary in English.

Previous summary (if any):
{summary_memory if summary_memory else "(None)"}

New conversation:
{recent_dialogue}

Output the new summary:
"""

    try:
        resp = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL_NAME,
                "prompt": summary_prompt,
                "stream": False
            },
            timeout=120
        )
        resp.raise_for_status()
        data = resp.json()
        new_summary = data.get("response", "").strip()
        if new_summary:
            session['summary'] = new_summary
            print("âœ… [Summary Updated]:", new_summary)
    except requests.RequestException as e:
        print(f"âš ï¸ Failed to generate summary: {e}")
    except Exception as e:
        print(f"âš ï¸ An unexpected error occurred during summary generation: {e}")


def get_llm_response_stream(participant_id: str, user_input: str):
    """
    å¤„ç†èŠå¤©é€»è¾‘å’Œ LLM å“åº”æµã€‚
    """
    session = get_session(participant_id)
    conversation_history = session['history']
    summary_memory = session['summary']

    # 1. å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å†å²è®°å½• (æ­¤å†å²è®°å½•åªä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œä¸å†™å…¥æ–‡ä»¶)
    conversation_history.append({"role": "user", "content": user_input})

    # --- æ„å»ºå®Œæ•´çš„æç¤ºè¯ (Prompt) ---
    full_prompt = ""

    if len(conversation_history) == 1:
        full_prompt += SYSTEM_PROMPT + "\n\n"

    if summary_memory:
        full_prompt += f"The following is a summary of previous conversation to help you understand context:\n{summary_memory}\n\n"

    for msg in conversation_history[-10:]:
        prefix = "User:" if msg["role"] == "user" else "AI:"
        full_prompt += f"{prefix} {msg['content']}\n"

    full_prompt += "AI:"

    session['full_prompt'] = full_prompt
    print("\n--- LLM Prompt ---")
    print(full_prompt)
    print("------------------\n")

    # --- æµå¼å“åº” ---
    full_ai_reply = ""
    try:
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": MODEL_NAME,
                "prompt": full_prompt,
                "stream": True
            },
            stream=True,
            timeout=300
        )
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                try:
                    json_line = line.decode('utf-8')
                    data = json.loads(json_line)
                    text_chunk = data.get("response", "")
                    if text_chunk:
                        full_ai_reply += text_chunk
                        yield text_chunk.encode('utf-8')
                    if data.get("done", False):
                        break
                except json.JSONDecodeError:
                    pass

    except requests.RequestException as e:
        yield f"âš ï¸ Failed connecting backend LLM: {e}".encode('utf-8')

    finally:
        if full_ai_reply:
            # 2. å°†å®Œæ•´çš„ AI å›å¤æ·»åŠ åˆ°å†å²è®°å½•
            conversation_history.append({"role": "ai", "content": full_ai_reply.strip()})

            # --- æ–°å¢: å¢åŠ å›åˆè®¡æ•° ---
            session['turn_count'] += 1

            if len(conversation_history) % (SUMMARY_INTERVAL * 2) == 0:
                generate_summary(session)
        print("âœ… Streaming Complete")